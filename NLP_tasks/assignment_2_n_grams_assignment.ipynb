{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What we want to do is build up a dictionary of N-grams, which are pairs, triplets or more (the N) of words that pop up in the training data, with the value being the number of times they showed up. After we have this dictionary, as a naive example we could actually predict sentences by just randomly choosing words within this dictionary and doing a weighted random sample of the connected words that are part of n-grams within the keys._\n",
    "\n",
    "_Lets see how far we can get with N-grams without outside resources._\n",
    "\n",
    "_We have a text file for [Pride and Prejudice from Project Gutenberg](https://www.gutenberg.org/ebooks/1342) stored as pg1342.txt in the same folder as our notebook, but also available online directly. Let's load the text to a string since it's only 701KB, which will fit in memory nowadays._\n",
    "\n",
    "_**Note** : If we wanted to be more memory efficient we should parse the text file on a line or character by character basis, storing as needed, etc._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775714 , The Project Gutenberg eBook of Pride and Prejudice ...\n"
     ]
    }
   ],
   "source": [
    "# Find the number links by looking on Project Gutenberg in the address bar for a book.\n",
    "books = {'Pride and Prejudice': '1342',\n",
    "         'Huckleberry Fin': '76',\n",
    "         'Sherlock Holmes': '1661'}\n",
    "\n",
    "book = books['Pride and Prejudice']\n",
    "\n",
    "f = open('gutenberg.txt', encoding='utf-8')\n",
    "txt = f.read()\n",
    "f.close()\n",
    "\n",
    "# See the number of characters and the first 50 characters to confirm it is there    \n",
    "print(len(txt), ',', txt[:50] , '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now lets split into words into a big list, splitting on anything non-alphanumeric [A-Za-z0-9] (as well as punctuation) and forcing everything lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126262\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words = re.split('[^A-Za-z0-9]+', txt.lower())\n",
    "words = list(filter(None, words)) # Remove empty strings\n",
    "\n",
    "# Print length of list\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams generation\n",
    "From this we can now generate N-grams, lets start with a 1-gram, basically the set of all the words\n",
    "\n",
    "**Note** : One could use a dictionary instead of a set and keeping count of the occurances gives word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6603\n",
      "['atoned', 'grosvenor', 'tremble', 'private', 'understood', 'painfully', 'read', 'selves', 'detestable', 'nonproprietary', 'bitterest', 'persisting', 'couple', 'useless', 'fashionable', 'inducements', 'stopping', 'pressingly', 'staggered', 'powerful']\n"
     ]
    }
   ],
   "source": [
    "# Create set of all unique words, this throws away any information about frequency however\n",
    "gram1 = set(words)\n",
    "\n",
    "print(len(gram1))\n",
    "\n",
    "# Print 20 elements of the set only\n",
    "print(list(gram1)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and get the 2-gram now, which is pairs of words. Let's have a quick look to see the last 10 and how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe to\n",
      "to our\n",
      "our email\n",
      "email newsletter\n",
      "newsletter to\n",
      "to hear\n",
      "hear about\n",
      "about new\n",
      "new ebooks\n"
     ]
    }
   ],
   "source": [
    "# See the last 10 pairs\n",
    "for i in range(len(words)-10, len(words)-1):\n",
    "    print(words[i], words[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, seems good, lets get all word pairs, and then generate a set of unique pairs from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126261\n",
      "55852\n",
      "[('ones', 'paid'), ('objection', 'said'), ('information', 'however'), ('is', 'obliged'), ('own', 'opinion'), ('her', 'going'), ('amiable', 'appearance'), ('awake', 'to'), ('encouraged', 'to'), ('winter', 'you'), ('often', 'think'), ('by', 'being'), ('speedily', 'pronounced'), ('had', 'detected'), ('in', 'half'), ('intermarriage', 'she'), ('perseveringly', 'by'), ('was', 'warmly'), ('first', 'was'), ('his', 'unpardonable')]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "print(len(word_pairs))\n",
    "\n",
    "gram2 = set(word_pairs)\n",
    "print(len(gram2))\n",
    "\n",
    "# Print 20 elements from gram2\n",
    "print(list(gram2)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams Frequency\n",
    "Okay, that was fun, but this isn't enough, we need frequency if we want to have any sense of probabilities, which is what N-grams are about. Instead of using sets, lets create a dictionary with counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4520), ('to', 4245), ('of', 3736), ('and', 3657), ('her', 2225), ('i', 2070), ('a', 2007), ('in', 1941), ('was', 1848), ('she', 1710), ('that', 1595), ('it', 1550), ('not', 1457), ('you', 1433), ('he', 1339), ('his', 1271), ('be', 1260), ('as', 1192), ('had', 1177), ('with', 1100)]\n"
     ]
    }
   ],
   "source": [
    "gram1 = dict()\n",
    "\n",
    "# Populate 1-gram dictionary\n",
    "for word in words:\n",
    "    if word in gram1:\n",
    "        gram1[word] += 1\n",
    "    else:\n",
    "        gram1[word] = 1 # Start a new entry with 1 count since saw it for the first time.\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram1 = sorted(gram1.items(), key=lambda item: -item[1])\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print([(word, freq) for word, freq in gram1[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Pride and Prejudice, the words 'the', 'to', 'of', and 'and' were the top four most common words. Sounds about right, not too interesting yet, lets see what happens with 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 498), (('to', 'be'), 445), (('in', 'the'), 401), (('i', 'am'), 303), (('mr', 'darcy'), 273), (('to', 'the'), 268), (('of', 'her'), 262), (('it', 'was'), 251), (('of', 'his'), 235), (('she', 'was'), 212), (('she', 'had'), 205), (('had', 'been'), 199), (('it', 'is'), 194), (('i', 'have'), 188), (('to', 'her'), 179), (('that', 'he'), 177), (('could', 'not'), 167), (('he', 'had'), 166), (('and', 'the'), 165), (('for', 'the'), 162)]\n"
     ]
    }
   ],
   "source": [
    "gram2 = dict()\n",
    "\n",
    "# Populate 2-gram dictionary\n",
    "for i in range(len(words)-1):\n",
    "    key = (words[i], words[i+1])\n",
    "    if key in gram2:\n",
    "        gram2[key] += 1\n",
    "    else:\n",
    "        gram2[key] = 1\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram2 = sorted(gram2.items(), key=lambda item: -item[1])\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print([(word, freq) for word, freq in gram2[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like \"of the\" and \"to be\" are the top two most common 2-grams, sounds good.\n",
    "\n",
    "##Â Next word prediction\n",
    "\n",
    "What can we do with this? Well lets see what happens if we take a random word from all the words, and build a sentence by just choosing the most common pair that has that word as it's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after\n"
     ]
    }
   ],
   "source": [
    "start_word = words[int(len(words)/4)]\n",
    "print(start_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just went ahead and chose the word that appears $1/4$ of the way into words, random enough.\n",
    "\n",
    "Now in a loop, iterate through the frequency list (most frequent first) and see if it matches the first word in a pair, if so, the next word is the second element in the word pair, and continue with that word. Stop after N words or the list does not contain that word.\n",
    "\n",
    "**Note** : gram2 is a list that contains (key,value) where key is a word pair (first, second),\n",
    "           so you need element[0][0] for first word and element [0][1] for second word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: after\n",
      "2-gram sentence: \" after a very much as to be so much as to be so much as to be so much as \"\n"
     ]
    }
   ],
   "source": [
    "def get2GramSentence(word, n = 50):\n",
    "    words = []\n",
    "    for i in range(n):\n",
    "        words.append(word)\n",
    "        # Find Next word\n",
    "        word = next((element[0][1] for element in gram2 if element[0][0] == word), None)\n",
    "        if not word:\n",
    "            break\n",
    "    return ' '.join(words)\n",
    "\n",
    "word = start_word\n",
    "print(\"Start word:\", word)\n",
    "\n",
    "print('2-gram sentence: \"', get2GramSentence(word, 20), '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gets stuck in a loop pretty much straight away. Not very interesting, try out other words and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and the whole party were to be so much as to be so much as to be so much as \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he had been so much as to be so much as to be so much as to be so much \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she was not be so much as to be so much as to be so much as to be so \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when she was not be so much as to be so much as to be so much as to be \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john with the whole party were to be so much as to be so much as to be so much \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never be so much as to be so much as to be so much as to be so much as \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i am sure i am sure i am sure i am sure i am sure i am sure i am \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how much as to be so much as to be so much as to be so much as to be \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print(\"Start word:\", word)\n",
    "    print('2-gram sentence: \"', get2GramSentence(word, 20), '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted random choice based on frequency\n",
    "\n",
    "**This is our simple probabilistic MLE N-gram model**\n",
    "\n",
    "Same thing. Okay, lets randomly choose from the subset of all 2grams that matches the first word, using a weighted-probability based on counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def weighted_choice(choices):\n",
    "    total = sum(w for c, w in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choices:\n",
    "        if upto + w > r:\n",
    "            return c\n",
    "        upto += w\n",
    "    \n",
    "def get2GramSentenceRandom(word, n = 50):\n",
    "    words = []\n",
    "    for i in range(n):\n",
    "        words.append(word)\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram2 if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and could not take any other about gracechurch street and assistance they are talking to have jane s disposition greater \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he chose to her life it to jane s approach was now and her to himself did it was an \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she cared i might bring was in it with offers of a salad and lydia s quick where i had \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when you were necessary business was told of questions to him alone mary and they were grieved she thought it \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john with colonel forster instantly passed together and as to refrain from any of the longbourn and they may lead \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never happened in her husband her stay quietly answered me more easily with a manner you to be explicit let \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i thought she called during dinner to leave town dear wickham for the sisters came forward and one of friends \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how mr darcy had lately fitted up stairs these but the course mrs bennet but in pursuit could not come \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print(\"Start word:\", word)\n",
    "    print('2-gram sentence: \"', get2GramSentenceRandom(word, 20), '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's way more interesting! Those are starting to look like sentences!\n",
    "\n",
    "Let's try a longer sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: it\n",
      "2-gram sentence: \" it earlier i can afford some passages which we begin freely available with his memory of schemes but it is gone jane in evident to hold her maternal feelings which her air of my dear said mrs bennet s marrying and sensible of the trouble of it her sister said \"\n"
     ]
    }
   ],
   "source": [
    "word = 'it'\n",
    "print(\"Start word:\", word)\n",
    "print('2-gram sentence: \"', get2GramSentenceRandom(word, 50), '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, lets see what happens when we go to N-grams above 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Tri-grams and more\n",
    "Okay, let's create a Ngram generator that can let us make ngrams of arbitrary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('i', 'do', 'not'), 62), (('i', 'am', 'sure'), 62), (('project', 'gutenberg', 'tm'), 57), (('as', 'soon', 'as'), 55), (('she', 'could', 'not'), 50), (('that', 'he', 'had'), 37), (('it', 'would', 'be'), 34), (('in', 'the', 'world'), 34), (('the', 'project', 'gutenberg'), 33), (('i', 'am', 'not'), 32), (('i', 'dare', 'say'), 31), (('it', 'was', 'not'), 30), (('could', 'not', 'be'), 30), (('that', 'he', 'was'), 29), (('mr', 'darcy', 's'), 29), (('that', 'it', 'was'), 28), (('on', 'the', 'subject'), 28), (('of', 'mr', 'darcy'), 27), (('would', 'have', 'been'), 27), (('as', 'well', 'as'), 27)]\n"
     ]
    }
   ],
   "source": [
    "def generateNgram(n=1):\n",
    "    gram = dict()\n",
    "    \n",
    "    # Some helpers to keep us crashing the PC for now\n",
    "    assert n > 0 and n < 100\n",
    "    \n",
    "    # Populate N-gram dictionary\n",
    "    for i in range(len(words)-(n-1)):\n",
    "        key = tuple(words[i:i+n])\n",
    "        if key in gram:\n",
    "            gram[key] += 1\n",
    "        else:\n",
    "            gram[key] = 1\n",
    "\n",
    "    # Turn into a list of (word, count) sorted by count from most to least\n",
    "    gram = sorted(gram.items(), key=lambda item: -item[1])\n",
    "    return gram\n",
    "\n",
    "trigram = generateNgram(3)\n",
    "# Print top 20 most frequent ngrams\n",
    "print(trigram[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "2-gram sentence: \n",
      "\"and down for caprice that she answered only let her friend and taking her friend says that head over the\"\n",
      "\n",
      "Start word: he\n",
      "2-gram sentence: \n",
      "\"he scarcely spoke joined them both of his determination i think it takes a most tractable creatures if i can\"\n",
      "\n",
      "Start word: she\n",
      "2-gram sentence: \n",
      "\"she wrote must date june for the monthly balls will you find that can you know what do cure the\"\n",
      "\n",
      "Start word: when\n",
      "2-gram sentence: \n",
      "\"when so but against mr collins having longbourn elizabeth had always so soon went away before i was nothing further\"\n",
      "\n",
      "Start word: john\n",
      "2-gram sentence: \n",
      "\"john told you in love it and even sir william was decided opinion in his father can get together very\"\n",
      "\n",
      "Start word: never\n",
      "2-gram sentence: \n",
      "\"never make my family from pressing and dazzling with some news well now employment and myself the short and i\"\n",
      "\n",
      "Start word: i\n",
      "2-gram sentence: \n",
      "\"i shall certainly spare half an exertion on his character she had chosen your disposition when i hope he replied\"\n",
      "\n",
      "Start word: how\n",
      "2-gram sentence: \n",
      "\"how it right of all if you may well said mrs forster instantly occurred to walk to know but he\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 3-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "3-gram sentence: \n",
      "\"and satisfy them and to her heart elizabeth you are quite alone mary or 1 f 1 f 3 you\"\n",
      "\n",
      "Start word: he\n",
      "3-gram sentence: \n",
      "\"he could boast of encouraging he shook hands in about half whisper that affection for my sister or associated in\"\n",
      "\n",
      "Start word: she\n",
      "3-gram sentence: \n",
      "\"she turned out and considering mr collins s a call upon my dear mr and this agreement you must give\"\n",
      "\n",
      "Start word: when\n",
      "3-gram sentence: \n",
      "\"when at her sense and that there were off two girls can be ashamed of her wickham with a mistress\"\n",
      "\n",
      "Start word: john\n",
      "3-gram sentence: \n",
      "\"john told mrs collins and even of miss elizabeth had myself with him before so tall person to elizabeth silently\"\n",
      "\n",
      "Start word: never\n",
      "3-gram sentence: \n",
      "\"never do and pointing out of its being in the affirmative and attended in a most pleasant aspect of little\"\n",
      "\n",
      "Start word: i\n",
      "3-gram sentence: \n",
      "\"i am not offensive to assistance in rejecting him in the proudest most unexpected and aunt did for a blush\"\n",
      "\n",
      "Start word: how\n",
      "3-gram sentence: \n",
      "\"how ardently i was in the instantaneous conviction of her griefs to live on his very pleasing though it by\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 4-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "4-gram sentence: \n",
      "\"and given them in everything i will for do not believed her exuberant spirits and then recollected my profession might\"\n",
      "\n",
      "Start word: he\n",
      "4-gram sentence: \n",
      "\"he left them within four per cents which had a compliment of what you of her former he so kind\"\n",
      "\n",
      "Start word: she\n",
      "4-gram sentence: \n",
      "\"she will not only to dance when he complimented him she had assured with his fair you be treating his\"\n",
      "\n",
      "Start word: when\n",
      "4-gram sentence: \n",
      "\"when he fears me happy on his cousin and delicate attentions and a fortnight they arose the possibility of their\"\n",
      "\n",
      "Start word: john\n",
      "4-gram sentence: \n",
      "\"john told you can i will say mr collins but there are not come to condemn him to quit the\"\n",
      "\n",
      "Start word: never\n",
      "4-gram sentence: \n",
      "\"never spoke to say i went along kitty and her place covered with only were to be in it not\"\n",
      "\n",
      "Start word: i\n",
      "4-gram sentence: \n",
      "\"i ought nor taste and asked her favour and yet he was dated a serious consequence as she had any\"\n",
      "\n",
      "Start word: how\n",
      "4-gram sentence: \n",
      "\"how he wishes for me i mention bingley i anything to the country advice and may meet them to speak\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 5-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "5-gram sentence: \n",
      "\"and in town last night but of passion to be written a royalty fee as she still possessed in the\"\n",
      "\n",
      "Start word: he\n",
      "5-gram sentence: \n",
      "\"he was overflowing with others and mary she was scarcely spoke to go no means certain types of it which\"\n",
      "\n",
      "Start word: she\n",
      "5-gram sentence: \n",
      "\"she would not have promoted its existence if elizabeth could appease her feel he applied and if you so highly\"\n",
      "\n",
      "Start word: when\n",
      "5-gram sentence: \n",
      "\"when i never be serviceable to whom we discuss or creating the two before supposed himself to be raised such\"\n",
      "\n",
      "Start word: john\n",
      "5-gram sentence: \n",
      "\"john told her family and do so abrupt was coming into the acquaintance to what she had first two and\"\n",
      "\n",
      "Start word: never\n",
      "5-gram sentence: \n",
      "\"never heard from the sisters on carrying away with others unhappy and her in whose astonishment and their boundary on\"\n",
      "\n",
      "Start word: i\n",
      "5-gram sentence: \n",
      "\"i am sure i am very agreeable than she very glad dearest jane i had mentioned to do all out\"\n",
      "\n",
      "Start word: how\n",
      "5-gram sentence: \n",
      "\"how gradually all see the storm was reasonable but the stream mr collins having a wink at length however your\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 6-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "6-gram sentence: \n",
      "\"and totally wrong i rely upon to pay no foundation at longbourn before continued talking to you have done but\"\n",
      "\n",
      "Start word: he\n",
      "6-gram sentence: \n",
      "\"he so little and making themselves it madam he answered mechanically to congratulate her tour of wickham imagine that you\"\n",
      "\n",
      "Start word: she\n",
      "6-gram sentence: \n",
      "\"she looked at the trunks were grieved at pemberley and kitty such attention avoiding mrs bennet and leave to something\"\n",
      "\n",
      "Start word: when\n",
      "6-gram sentence: \n",
      "\"when i may be a user to wait your mother and charlotte i conclude for his father elizabeth and his\"\n",
      "\n",
      "Start word: john\n",
      "6-gram sentence: \n",
      "\"john with mrs bennet that moment she will fight wickham and he must give implicit confidence had the whole party\"\n",
      "\n",
      "Start word: never\n",
      "6-gram sentence: \n",
      "\"never can give her illness if i could not follow wherever you could not absolutely certain she saw that she\"\n",
      "\n",
      "Start word: i\n",
      "6-gram sentence: \n",
      "\"i ever forget as well as they should do him into her nothing child she is very expressive of having\"\n",
      "\n",
      "Start word: how\n",
      "6-gram sentence: \n",
      "\"how could furnish conversation a second time mr collins s spirits mrs long absence she would be the accustomed to\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 7-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "7-gram sentence: \n",
      "\"and to town and trusting their taking the excellency of both had better of felicity oh lydia determined to a\"\n",
      "\n",
      "Start word: he\n",
      "7-gram sentence: \n",
      "\"he must excuse my father taking in kent his character but poor consolation then explained some exaggeration the road with\"\n",
      "\n",
      "Start word: she\n",
      "7-gram sentence: \n",
      "\"she dared not an instant and often talk to resent it is thereby complete the object and she is more\"\n",
      "\n",
      "Start word: when\n",
      "7-gram sentence: \n",
      "\"when she might as made the sound and though it possible and as by letting them into the best could\"\n",
      "\n",
      "Start word: john\n",
      "7-gram sentence: \n",
      "\"john with a something for his returning the very important to see her hours learn to those who was too\"\n",
      "\n",
      "Start word: never\n",
      "7-gram sentence: \n",
      "\"never looks and i were not certain you can be used to georgiana was interrupted again to live i have\"\n",
      "\n",
      "Start word: i\n",
      "7-gram sentence: \n",
      "\"i have all others his attention your ladyship received her opinion constitute my dear lizzy only on the younger girls\"\n",
      "\n",
      "Start word: how\n",
      "7-gram sentence: \n",
      "\"how you would go to project gutenberg tm work with regard for their appearing in london would be frightened by\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 8-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "8-gram sentence: \n",
      "\"and pray go to think she was welcome elizabeth passed by herself forward and we begin said lately saved a\"\n",
      "\n",
      "Start word: he\n",
      "8-gram sentence: \n",
      "\"he becomes really is a different when mary king were married your sisters must be curiosity satisfied and they did\"\n",
      "\n",
      "Start word: she\n",
      "8-gram sentence: \n",
      "\"she saw her sisters may well as the little else and whose manners grew fainter and his daughter s countenance\"\n",
      "\n",
      "Start word: when\n",
      "8-gram sentence: \n",
      "\"when you can write myself is under frequent as of all the party who had any such happy have suffered\"\n",
      "\n",
      "Start word: john\n",
      "8-gram sentence: \n",
      "\"john with her mother how it she did not appear insufficient for and confidence gave it cannot even of flattering\"\n",
      "\n",
      "Start word: never\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-gram sentence: \n",
      "\"never without a spot or to add that we may only imagine me when it she should infinitely better that\"\n",
      "\n",
      "Start word: i\n",
      "8-gram sentence: \n",
      "\"i do you hold out as she has a second daughter elizabeth who that had ruined the way in spite\"\n",
      "\n",
      "Start word: how\n",
      "8-gram sentence: \n",
      "\"how i cannot wonder who must be very part of what congratulations to be placed himself to see if not\"\n",
      "\n",
      "***************************************************************************\n",
      "Generating 9-gram list...\n",
      "Done\n",
      "Start word: and\n",
      "9-gram sentence: \n",
      "\"and if he was consequently was hurt he offered to one seemed to have two hours giving her mind that\"\n",
      "\n",
      "Start word: he\n",
      "9-gram sentence: \n",
      "\"he was the room elizabeth said in this ebook pride which reflection in their father would be alone his plan\"\n",
      "\n",
      "Start word: she\n",
      "9-gram sentence: \n",
      "\"she was short i am growing more like your admiration and i wonder and backed by design nonsense of him\"\n",
      "\n",
      "Start word: when\n",
      "9-gram sentence: \n",
      "\"when he chooses nobody else i have got some cause section 4 information page she could originate elizabeth colouring i\"\n",
      "\n",
      "Start word: john\n",
      "9-gram sentence: \n",
      "\"john told her be mercenary if she had nothing had been calculated using or because he had been most thankfully\"\n",
      "\n",
      "Start word: never\n",
      "9-gram sentence: \n",
      "\"never forget all like you must have been an advantage the problem 1 1 f who has not know sister\"\n",
      "\n",
      "Start word: i\n",
      "9-gram sentence: \n",
      "\"i hope you he could not have saved you and distribution of seeing mr bennet they saw no such an\"\n",
      "\n",
      "Start word: how\n",
      "9-gram sentence: \n",
      "\"how is an hour s ungraciousness made an opportunity of her feelings as if miss bingley were carried to rosings\"\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def getNGramSentenceRandom(gram, word, n = 50):\n",
    "    words = []\n",
    "    for i in range(n):\n",
    "        words.append(word)\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "for n in range(2,10):\n",
    "    # Generate ngram list\n",
    "    print(f\"Generating {n}-gram list...\")\n",
    "    ngram = generateNgram(n)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    # Try out a bunch of sentences\n",
    "    for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "        print(\"Start word:\", word)\n",
    "        print(f'{n}-gram sentence: \\n\"{getNGramSentenceRandom(ngram, word, 20)}\"')\n",
    "        print()\n",
    "        \n",
    "    print('***************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences produced by higher-level N-gram looks almost like normal sentences if you squint a little!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additive Smoothin\n",
    "\n",
    "Let's try and adjust our current function of create n_grams with laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 4 gram with laplace smoothing \n",
    "\n",
    "def create_gram_laplace(vocab, n=1):\n",
    "    gram_dict = dict()\n",
    "    \n",
    "    assert n > 0 and n < 25 # keep assertions and safety more strictly\n",
    "    \n",
    "    for i in range(len(vocab) - n - 1):\n",
    "        gram = tuple(vocab[i:i+n])\n",
    "        candidate = gram_dict.get(gram, 1)\n",
    "        gram_dict[gram] = candidate + 1\n",
    "        \n",
    "    sorted_gram_dict = sorted(gram_dict.items(), key=lambda item: -item[1])\n",
    "    return sorted_gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_gram = create_gram_laplace(words, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('i', 'do', 'not', 'know'), 20),\n",
       " (('project', 'gutenberg', 'tm', 'electronic'), 19),\n",
       " (('at', 'the', 'same', 'time'), 17),\n",
       " (('the', 'rest', 'of', 'the'), 16),\n",
       " (('i', 'am', 'sure', 'i'), 16),\n",
       " (('in', 'the', 'course', 'of'), 15),\n",
       " (('lady', 'catherine', 'de', 'bourgh'), 15),\n",
       " (('as', 'soon', 'as', 'they'), 14),\n",
       " (('her', 'uncle', 'and', 'aunt'), 14),\n",
       " (('mr', 'and', 'mrs', 'gardiner'), 14),\n",
       " (('the', 'project', 'gutenberg', 'tm'), 14),\n",
       " (('the', 'project', 'gutenberg', 'literary'), 14),\n",
       " (('project', 'gutenberg', 'literary', 'archive'), 14),\n",
       " (('gutenberg', 'literary', 'archive', 'foundation'), 14),\n",
       " (('gutenberg', 'tm', 'electronic', 'works'), 13),\n",
       " (('if', 'you', 'do', 'not'), 12),\n",
       " (('in', 'the', 'united', 'states'), 11),\n",
       " (('of', 'the', 'project', 'gutenberg'), 11),\n",
       " (('for', 'the', 'sake', 'of'), 11),\n",
       " (('as', 'soon', 'as', 'he'), 10)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_gram[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('newsletter', 'to', 'hear', 'about'), 2),\n",
       " (('email', 'newsletter', 'to', 'hear'), 2),\n",
       " (('our', 'email', 'newsletter', 'to'), 2),\n",
       " (('to', 'our', 'email', 'newsletter'), 2),\n",
       " (('subscribe', 'to', 'our', 'email'), 2),\n",
       " (('to', 'subscribe', 'to', 'our'), 2),\n",
       " (('how', 'to', 'subscribe', 'to'), 2),\n",
       " (('and', 'how', 'to', 'subscribe'), 2),\n",
       " (('ebooks', 'and', 'how', 'to'), 2),\n",
       " (('new', 'ebooks', 'and', 'how'), 2),\n",
       " (('our', 'new', 'ebooks', 'and'), 2),\n",
       " (('produce', 'our', 'new', 'ebooks'), 2),\n",
       " (('help', 'produce', 'our', 'new'), 2),\n",
       " (('to', 'help', 'produce', 'our'), 2),\n",
       " (('how', 'to', 'help', 'produce'), 2),\n",
       " (('foundation', 'how', 'to', 'help'), 2),\n",
       " (('archive', 'foundation', 'how', 'to'), 2),\n",
       " (('literary', 'archive', 'foundation', 'how'), 2),\n",
       " (('make', 'donations', 'to', 'the'), 2)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we should have at least 2 occurences to say the least\n",
    "four_gram[:-20:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "4-gram sentence: \n",
      "\"and determining probabilities and our dear jane her daughter endeavoured to have been a mother up nonproprietary or 1 e\"\n",
      "\n",
      "Start word: he\n",
      "4-gram sentence: \n",
      "\"he was going away without being so there is adjusting her sister s delight to him before you were obliged\"\n",
      "\n",
      "Start word: she\n",
      "4-gram sentence: \n",
      "\"she rambled about him and amiable charming daughter let down and asking me when the wife yes my sentiments you\"\n",
      "\n",
      "Start word: when\n",
      "4-gram sentence: \n",
      "\"when i think meanly of its injustice of my other alteration for the evening elizabeth where she who followed her\"\n",
      "\n",
      "Start word: john\n",
      "4-gram sentence: \n",
      "\"john with a peep at the valley as much affection will not engross her again with her eye power to\"\n",
      "\n",
      "Start word: never\n",
      "4-gram sentence: \n",
      "\"never resent the least more interesting intelligence of doing a mixture of the moral if you reasons to miss bennet\"\n",
      "\n",
      "Start word: i\n",
      "4-gram sentence: \n",
      "\"i talked of distinguishing the whole evening but charlotte says that jane which must be hurt and elizabeth soon lydia\"\n",
      "\n",
      "Start word: how\n",
      "4-gram sentence: \n",
      "\"how can be able to see jane as jane i had the best of objection to make my regard with\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see what kind of sentences we can generate with these 4-gram we ve got\n",
    "\n",
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "        print(\"Start word:\", word)\n",
    "        print(f'{4}-gram sentence: \\n\"{getNGramSentenceRandom(four_gram, word, 20)}\"')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
